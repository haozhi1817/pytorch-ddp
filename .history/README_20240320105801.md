# pytorch-ddp
一个简单的单机多卡DDP demo

## 基础知识

### DDP与DP

DDP与DP的核心差异在于前者属于多进程后者属于多线程。至于DDP采用ring-all-reduce来处理更新量，DP采用PS模式来处理更新量，个人研究不多，在我浅薄的理解中，仅仅是因为torch的DP采用PS模式，而并不是DP仅有PS这一种实现模式。

> 注意，这里提到的是更新量而非梯度，因为不同的optimizer会根据梯度计算不同的更新量，例如SAG的更新量等于梯度乘以学习率，其他优化器则还包括动量，二阶导等多种形式。这里提到这个差异，并非为了强调二者有何不同，恰恰是为了强调二者实际上是相同的。

### 梯度累加

了解梯度累加可以更好地了解多进程训练的本质以及DDP中关于loss需要除以进程数的原因。

### 进程通信

python的多进程之间一般是不共享数据的，所以需要借助其他工具来进行进程通信。相比于最简单的point2point通信，pytorch采用了collective communication也即集合通信，这是一种all2all的通信方式。这种通信方式支持了一些基础操作如：gather, reduce, broadcast, all_gather, all_reduce等。可以通过下列代码尝试观察这些操作的实现方式与效果：
```bash
cd utils
python dist_operator.py
```
进程通信需要先初始化进程，即代码中的**dist.init_process_group**，初始化的时候需要选择通信后端，pytorch支持多种后端，其中nccl后端仅支持显卡之间的通信，cpu之间的通信需要选择gloo后端，上述代码中的数据与操作均在cpu上完成，因此初始化进程时选择了gloo后端。

### torchrun

pytorch支持多种方式启动多进程训练，最基本的方式为**torch.multiprocessing.set_start_method("spawn")**,还有一种弹性启动的方式: **torch.distributed.lunch**，但是这种方式正在被启用，最新的启动方式为: **torchrun**.所谓的弹性启动即可以实现当程序检测到某一个进程因为某些原因死亡后，程序可以重新启动整个程序。这个特性针对大量节点的情况非常有用。可以通过下述代码来尝试这个功能：
```bash
cd utils
turchrun --nproc_per_node=2 lunch_try.py
```
通过这种方式启动的多进程程序，会自动生成系统变量**WORLD_SIZE**, **RANK**, **LOCAL_RANK**，其中对我们最重要的是**LOCAL_RANK**, 这个数字代表了当前节点（机器）上的某个gpu的id。可以通过**torch.device(local_rank)**
或者**`tensor.to(f'cuda:{local_rank}')`**把数据转移到当前进程所在的gpu上。